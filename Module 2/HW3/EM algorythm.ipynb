{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Математический вывод ЕМ (Expectation-maximization) алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим вероятностную модель смеси нормальных распределений (в общем случае может быть рассмотрено другое семейство распределений):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x) = \\sum_{z}p(x, z) = \\sum_{k=1}^K w_k N(x | \\theta_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь \n",
    "\n",
    "$N(x | \\theta_k)$ - плотность некоторого ($k$-ого) распределения из семейства нормальных\n",
    "\n",
    "$K$ - количество распределений из семейства нормальных в смеси\n",
    "\n",
    "$z$ - скрытая переменная, $K$-мерный бинарный случайный вектор, ровно одна\n",
    "компонента которого равна единице (скрытая переменная). Определяет какое именно распределение из множества $K$ в смеси было взято\n",
    "\n",
    "$w_k$ - приорная вероятность взять то или иное распределение из семейства. Вероятность того, что единице будет равна $k$-я компонента вектора $z$\n",
    "\n",
    "$x$ - объект из выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим вероятностную модель с наблюдаемыми переменными X, параметрами $\\theta$ и скрытыми переменными $z$, для которой задано полное правдоподобие $log p(X, Z | \\Theta)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log p(X, Z | \\Theta) = \\sum_{i=1}^l \\sum_{k=1}^K z_{ik} \\big\\{ log w_k + log N(x_i | \\theta_k) \\big\\} $$\n",
    "$l$ - количество объектов в выборе X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EM-алгоритм решает задачу максимизации полного правоподобия путем попеременной оптимизации по параметрам и по скрытым переменным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг E\n",
    "Воспользуемся байесовским подходом, зафиксируем вектор параметров $\\Theta^{old}$ и вычислим апостериорное распределение на скрытых переменных $p(Z | X, \\Theta^{old})$:\n",
    "\n",
    "$$p(Z | X, \\Theta^{old}) = \\frac{p(X, Z | \\Theta^{old})}{p(X |\\Theta^{old})}$$\n",
    "\n",
    "Правая часть равенства пропорциональна:\n",
    "\n",
    "$$\\prod_{i=1}^l \\prod_{k=1}^K w_k^{old} N (x_i | \\mu_k^{old}, \\Sigma_k^{old})$$\n",
    "\n",
    "А данное распределение распадается в произведение распределений, соответствующих отдельным объектам $p(z_i| x_i, \\Theta)$:\n",
    "\n",
    "$$p(Z | X, \\Theta^{old}) = \\prod_{i=1}^l  p(z_i| x_i, \\Theta^{old})$$\n",
    "\n",
    "Из записанного следует, что величины $z_i$ независимы при известных объектах $x_i$. Вектор $z_i$ имеет K возможных значений. Запишем   все их вероятности, воспользовавшись формулой Байеса:\n",
    "\n",
    "$$g_{ik} = p(z_{ik}=1 | x_i, \\Theta^{old}) =  \\frac{p(z_{ik}=1) p(x_i | z_{ik}=1, \\Theta^{old})}{\\sum_{j=1}^K p(z_{ij}=1) p(x_i | z_{ij}=1, \\Theta^{old})}$$\n",
    "\n",
    "\n",
    "\n",
    "Запишем теперь матожидание логарифма полного правдоподобия по всем возможным значениям\n",
    "скрытых переменных $Z$ с весами, равными апостериорным вероятностям этих переменных $p(Z | X, \\Theta^{old})$:\n",
    "\n",
    "$$Q(\\Theta, \\Theta^{old}) = E_{Z} log p(X, Z | \\Theta) = E_Z \\sum_{i=1}^l \\sum_{k=1}^K z_{ik} \\big\\{log w_k + log N(x_i | \\mu_k, \\Sigma_k)\\big\\} = \\sum_{i=1}^l \\sum_{k=1}^K E_Z [z_{ik}] \\big\\{log w_k + log N(x_i | \\mu_k, \\Sigma_k)\\big\\}  $$\n",
    "\n",
    "Замтим, что:\n",
    "$$E_Z [z_{ik}] = 1 * p(z_{ik}=1 | x_i, \\Theta) + 0 * p(z_{ik}=0 | x_i, \\Theta) = g_{ik}$$\n",
    "\n",
    "Тогда \n",
    "\n",
    "$$Q(\\Theta, \\Theta^{old}) = \\sum_{i=1}^l \\sum_{k=1}^K g_{ik} \\big\\{log w_k + log N(x_i | \\mu_k, \\Sigma_k)\\big\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг M\n",
    "Новый вектор параметров находится как максимизатор матожидания из шага Е по параметрам тета:\n",
    "\n",
    "$$\\Theta^{new} = argmax_{\\Theta} E_{Z} log p(X, Z | \\Theta) $$\n",
    "\n",
    "Для этого продифференцируем функционал, полученный на шаге Е (логарифм полного правдоподобия), и затем все частные производные приравняем к 0. Из полученной системы уравнений мы найдем формулы для шага М с учетом ограничения $\\sum_{k=1}^K w_k = 1$:\n",
    "\n",
    "$$\\frac{dQ(\\Theta, \\Theta^{old})}{d w_k} = \\frac{1}{w_k} \\sum_{i=1}^l g_{ik} - l = 0 $$\n",
    "\n",
    "$$ w_k = \\frac{1}{l} \\sum_{i=1}^l g_{ik} $$\n",
    "\n",
    "И аналогично для матицы ковариаций и центров:\n",
    "\n",
    "$$\\mu_k = \\frac{1}{l w_k} \\sum_{i=1}^l g_{ik} x_i $$\n",
    "\n",
    "$$\\Sigma_k =  \\frac{1}{l w_k} \\sum_{i=1}^l g_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
