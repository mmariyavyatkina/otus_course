{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Избавление от полисемии в предобученном W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем для примера 2 предложения, в каждом из которых будет содержаться определенное слово, но в разных значениях. Натренируем модель и покажем, что возникает проблема полисемии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:29:28: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "spacy.load(\"en\")\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предложения возьмем следующие:\n",
    "    \n",
    "1) “My friend is considering to take a loan from a bank.”\n",
    "\n",
    "2) “Rainfall caused Rhine river to overflow it’s bank. ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[\"my\", \"friend\", \"considering\", \"to\", \"take\", \"a\", \"loan\", \"from\", \"a\", \"bank\"],\n",
    " [\"rainfall\", \"caused\", \"neckar\", \"river\", \"to\", \"overflow\", \"bank\", \"\", \"\", \"\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:29:29: collecting all words and their counts\n",
      "INFO - 13:29:29: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 13:29:29: collected 32 word types from a corpus of 20 words (unigram + bigrams) and 2 sentences\n",
      "INFO - 13:29:29: using 32 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 13:29:29: source_vocab length 32\n",
      "INFO - 13:29:29: Phraser built with 0 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(tokens, min_count=30, progress_per=10000) # train full Phrases model\n",
    "bigram = Phraser(phrases) # cut down memory consumption of Phrases\n",
    "sentences = bigram[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=0, # 20\n",
    "                     workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:29:29: collecting all words and their counts\n",
      "INFO - 13:29:29: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 13:29:29: collected 15 word types from a corpus of 20 raw words and 2 sentences\n",
      "INFO - 13:29:29: Loading a fresh vocabulary\n",
      "INFO - 13:29:29: effective_min_count=1 retains 15 unique words (100% of original 15, drops 0)\n",
      "INFO - 13:29:29: effective_min_count=1 leaves 20 word corpus (100% of original 20, drops 0)\n",
      "INFO - 13:29:29: deleting the raw counts dictionary of 15 items\n",
      "INFO - 13:29:29: sample=6e-05 downsamples 15 most-common words\n",
      "INFO - 13:29:29: downsampling leaves estimated 0 word corpus (3.0% of prior 20)\n",
      "INFO - 13:29:29: estimated required memory for 15 words and 300 dimensions: 25500 bytes\n",
      "INFO - 13:29:29: resetting layer weights\n",
      "INFO - 13:29:29: training model with -1 workers on 15 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=0 window=2\n",
      "INFO - 13:29:29: EPOCH - 1 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 1 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 2 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 2 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 3 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 3 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 4 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 4 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 5 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 5 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 6 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 6 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 7 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 7 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 8 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 8 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 9 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 9 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 10 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 10 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 11 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 11 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 12 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 12 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 13 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 13 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 14 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 14 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 15 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 15 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 16 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 16 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 17 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 17 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 18 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 18 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 19 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 19 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 20 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 20 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 21 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 21 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 22 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 22 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 23 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 23 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 24 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 24 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 25 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 25 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 26 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 26 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 27 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 27 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 28 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 28 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 29 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 29 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 30 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 30 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 31 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 31 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 32 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 32 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 33 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 33 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 34 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 34 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 35 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 35 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 36 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 36 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 37 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 37 : supplied example count (0) did not equal expected count (2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 13:29:29: EPOCH - 38 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 38 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 39 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 39 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 40 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 40 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 41 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 41 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 42 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 42 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 43 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 43 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 44 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 44 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 45 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 45 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 46 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 46 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 47 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 47 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 48 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 48 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 49 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 49 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 50 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 50 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 51 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 51 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 52 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 52 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 53 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 53 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 54 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 54 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 55 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 55 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 56 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 56 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 57 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 57 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 58 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 58 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 59 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 59 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 60 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 60 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 61 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 61 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 62 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 62 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 63 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 63 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 64 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 64 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 65 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 65 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 66 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 66 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 67 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 67 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 68 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 68 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 69 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 69 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 70 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 70 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 71 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 71 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 72 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 72 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 73 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 73 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 74 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 74 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 75 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 75 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 76 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 76 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 77 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 77 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 78 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 78 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 79 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 13:29:29: EPOCH - 79 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 80 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 80 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 81 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 81 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 82 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 82 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 83 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 83 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 84 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 84 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 85 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 85 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 86 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 86 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 87 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 87 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 88 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 88 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 89 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 89 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 90 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 90 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 91 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 91 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 92 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 92 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 93 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 93 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 94 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 94 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 95 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 95 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 96 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 96 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 97 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 97 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 98 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 98 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 99 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 99 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: EPOCH - 100 : training on 0 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "WARNING - 13:29:29: EPOCH - 100 : supplied example count (0) did not equal expected count (2)\n",
      "INFO - 13:29:29: training on a 0 raw words (0 effective words) took 0.7s, 0 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.build_vocab(tokens, progress_per=100)\n",
    "w2v_model.train(tokens, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n",
    "# w2v_model=w2v_model.wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель мы натренировали, теперь покажем проблему полисемии для слова \"bank\". Это слово может быть использовано как в значении \"банк\", так и в значении \"берег озера\". Тем не менее в Word2vec этому слову для всех его значений будет поставлен в соответствие только один вектор размером, указанным нами ранее (size=300):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3996725e-03, -1.1365307e-03, -2.7173077e-04,  4.7763996e-04,\n",
       "        1.6365285e-03,  3.5416731e-05,  6.3574029e-04, -1.3060764e-04,\n",
       "       -9.2497311e-04, -1.3173001e-03], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['bank'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bank', 'bank')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0][9], tokens[1][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор для слова \"bank\" = \"банк\" из первого предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3996725e-03, -1.1365307e-03, -2.7173077e-04,  4.7763996e-04,\n",
       "        1.6365285e-03,  3.5416731e-05,  6.3574029e-04, -1.3060764e-04,\n",
       "       -9.2497311e-04, -1.3173001e-03], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector(tokens[0][9])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вектор для слова \"bank\" = \"берег озера\" из второго предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3996725e-03, -1.1365307e-03, -2.7173077e-04,  4.7763996e-04,\n",
       "        1.6365285e-03,  3.5416731e-05,  6.3574029e-04, -1.3060764e-04,\n",
       "       -9.2497311e-04, -1.3173001e-03], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector(tokens[1][6])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, векторы совпадают (для краткости было выведено 10 первых координат, но можно было сравнить также и остальные 290). Таким образом, нескольким значениям одного слова соответствует один вектор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можем сделать:\n",
    "\n",
    "* Используем несколько представлений слова, чтобы обнаружить полисемию\n",
    "* Кластеризуем контексты, чтобы обнаружить полисемию; затем переприсвоим лейблы словам, чтобы у каждого смысла для слова был свой вектор\n",
    "* Находим альтернативные представления для слов при помощи постобработки нормальных Word2vec векторов\n",
    "\n",
    "Источник: https://stackoverflow.com/questions/51330549/using-word2vec-for-polysemy-solving-problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем примере мы используем модель ELMo (Embeddings from Language Models):\n",
    "\n",
    "* Если Word2vec использует на входе слова, то ELMo берет для этого комбинации букв\n",
    "* В Word2vec для построения ембедингов слов используется 1 скрытый слой (с выходным - 2 слоя), в то время как в ELMo для сети слоя 3: (1) Сверточные нейронные сети (CNN), (2) Двунаправленная сеть с долгой краткосрочной памятью (LSTM), (3) Ембединги\n",
    "* ELMo строит ембединги динамически, ориентируясь на близлежащие буквы\n",
    "\n",
    "На вход первому слою (CNN) подается последовательность комбинаций букв из предложения, Bi-directed LSTM (слой 2) для каждого слова учитывает его окружение слева и справа, чтобы затем ембединги могли объединить результат уже с учетом контекста слова.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0717 14:59:47.388398 4643038656 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "tokens_length = [10, 7]\n",
    "embeddings = elmo(\n",
    "        inputs={\n",
    "            \"tokens\": tokens,\n",
    "            \"sequence_len\": tokens_length\n",
    "        },\n",
    "        signature=\"tokens\",\n",
    "        as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "elmo_embeddings = session.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02096611,  0.23486409,  0.00171002,  0.21556687,  0.68794274,\n",
       "        0.6366527 , -0.18037347,  0.2796358 , -0.04736871,  0.41843903],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embeddings[0][9][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3807178 ,  0.60629106,  0.22663993,  0.07565593,  0.2112593 ,\n",
       "        0.32979885, -0.3203162 , -0.11011641,  0.48975784, -0.00121106],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_embeddings[1][6][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно уже из первых координат векторов, контекст учесть удалось: для первого и второго предложения слово \"bank\" имеет свой вектор для своего контекста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
